{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST USPS SVM, RF, Logistic Regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fIgayMnPUNy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from urllib import request\n",
        "import gzip\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm_notebook\n",
        "from keras.utils import np_utils\n",
        "from sklearn.metrics import confusion_matrix\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zReccPapPWOG",
        "colab_type": "code",
        "outputId": "57be4225-f4b4-4c98-fda4-53c58e2f2e3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#MNIST Data download and processing\n",
        "filename = [\n",
        "[\"training_images\",\"train-images-idx3-ubyte.gz\"],\n",
        "[\"test_images\",\"t10k-images-idx3-ubyte.gz\"],\n",
        "[\"training_labels\",\"train-labels-idx1-ubyte.gz\"],\n",
        "[\"test_labels\",\"t10k-labels-idx1-ubyte.gz\"]\n",
        "]\n",
        "\n",
        "def download_mnist():\n",
        "    base_url = \"http://yann.lecun.com/exdb/mnist/\"\n",
        "    for name in filename:\n",
        "        print(\"Downloading \"+name[1]+\"...\")\n",
        "        request.urlretrieve(base_url+name[1], name[1])\n",
        "    print(\"Download complete.\")\n",
        "\n",
        "def save_mnist():\n",
        "    mnist = {}\n",
        "    for name in filename[:2]:\n",
        "        with gzip.open(name[1], 'rb') as f:\n",
        "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1,28*28)\n",
        "    for name in filename[-2:]:\n",
        "        with gzip.open(name[1], 'rb') as f:\n",
        "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "    with open(r\"mnist.pkl\", 'wb') as f:\n",
        "        pickle.dump(mnist,f)\n",
        "    print(\"Save complete.\")\n",
        "\n",
        "def init():\n",
        "    download_mnist()\n",
        "    save_mnist()\n",
        "\n",
        "def load():\n",
        "    with open(r\"mnist.pkl\",'rb') as f:\n",
        "        mnist = pickle.load(f)\n",
        "        print(mnist.keys())\n",
        "    #return mnist[\"training_images\"], mnist[\"training_labels\"], mnist[\"test_images\"], mnist[\"test_labels\"]\n",
        "    return mnist\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    init()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading train-images-idx3-ubyte.gz...\n",
            "Downloading t10k-images-idx3-ubyte.gz...\n",
            "Downloading train-labels-idx1-ubyte.gz...\n",
            "Downloading t10k-labels-idx1-ubyte.gz...\n",
            "Download complete.\n",
            "Save complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hji00JgoPaFu",
        "colab_type": "code",
        "outputId": "2b927214-bb78-47bb-fcad-b08b6a2cf3e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')\n",
        "\n",
        "USPSMat  = []\n",
        "USPSTar  = []\n",
        "curPath  = r'/content/gdrive/My Drive/Colab Notebooks/Numerals/'\n",
        "savedImg = [] \n",
        "\n",
        "for j in range(0,10):\n",
        "    curFolderPath = curPath + '/' + str(j)\n",
        "    imgs =  os.listdir(curFolderPath)\n",
        "    for img in imgs:\n",
        "        curImg = curFolderPath + '/' + img\n",
        "        if curImg[-3:] == 'png':\n",
        "            img = Image.open(curImg,'r')\n",
        "            img = img.resize((28, 28))\n",
        "            savedImg = img\n",
        "            imgdata = (255-np.array(img.getdata()))/255\n",
        "            USPSMat.append(imgdata)\n",
        "            USPSTar.append(j)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wULaB31ZPgOg",
        "colab_type": "code",
        "outputId": "2ba9bfdc-2b05-433d-c4d5-fa6b0d8807da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#MNIST Data\n",
        "MNist_Dataset = load()\n",
        "Mnist_TrainingData = MNist_Dataset[\"training_images\"][:50000]\n",
        "Mnist_TrainingTarget = MNist_Dataset[\"training_labels\"][:50000]\n",
        "\n",
        "#MNIST TestData\n",
        "Mnist_TestingData = MNist_Dataset[\"training_images\"][50000:60000]\n",
        "Mnist_TestingTarget = MNist_Dataset[\"training_labels\"][50000:60000]\n",
        "\n",
        "#USPS Data\n",
        "USPS_TestingData = pd.DataFrame(USPSMat)\n",
        "USPS_TargetData = pd.DataFrame(USPSTar)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['training_images', 'test_images', 'training_labels', 'test_labels'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPKgwNbnwTe_",
        "colab_type": "code",
        "outputId": "cfc22050-cc1d-4dba-f7f3-dc25b63d313b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8887
        }
      },
      "source": [
        "#IMPLEMENTING LOGISTIC REGRESSION\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "def init_weights(shape):\n",
        "    return tf.Variable(tf.random_normal(shape, stddev=0.01))\n",
        "\n",
        "\n",
        "def model(X, w):\n",
        "    return tf.matmul(X, w) \n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "trX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels\n",
        "\n",
        "X = tf.placeholder(\"float\", [None, 784]) # Create symbolic variables\n",
        "Y = tf.placeholder(\"float\", [None, 10])\n",
        "\n",
        "w = init_weights([784, 10]) #We need shared variable weight matrix for logistic regression\n",
        "\n",
        "py_x = model(X, w)\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=py_x, labels=Y)) #Computes mean cross entropy by applying softmax internally\n",
        "train_op = tf.train.GradientDescentOptimizer(0.05).minimize(cost) #Construct optimizer\n",
        "predict_op = tf.argmax(py_x, 1) # at predict time, evaluate the argmax of the logistic regression\n",
        "\n",
        "# Launch the graph in a session\n",
        "with tf.Session() as sess:\n",
        "    # you need to initialize all variables\n",
        "    tf.global_variables_initializer().run()\n",
        "\n",
        "    for i in range(500):\n",
        "        for start, end in zip(range(0, len(trX), 128), range(128, len(trX)+1, 128)):\n",
        "            sess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end]})\n",
        "        print(i, np.mean(np.argmax(teY, axis=1) ==\n",
        "                         sess.run(predict_op, feed_dict={X: teX})))\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "0 0.8839\n",
            "1 0.8971\n",
            "2 0.9028\n",
            "3 0.9071\n",
            "4 0.9093\n",
            "5 0.9103\n",
            "6 0.9116\n",
            "7 0.9132\n",
            "8 0.9145\n",
            "9 0.9156\n",
            "10 0.9158\n",
            "11 0.9163\n",
            "12 0.9166\n",
            "13 0.9171\n",
            "14 0.9175\n",
            "15 0.9177\n",
            "16 0.918\n",
            "17 0.9184\n",
            "18 0.919\n",
            "19 0.9196\n",
            "20 0.9197\n",
            "21 0.9199\n",
            "22 0.92\n",
            "23 0.9201\n",
            "24 0.9204\n",
            "25 0.9204\n",
            "26 0.9206\n",
            "27 0.921\n",
            "28 0.9211\n",
            "29 0.9218\n",
            "30 0.9218\n",
            "31 0.9218\n",
            "32 0.9215\n",
            "33 0.9214\n",
            "34 0.9214\n",
            "35 0.9214\n",
            "36 0.9214\n",
            "37 0.9214\n",
            "38 0.9218\n",
            "39 0.9219\n",
            "40 0.9219\n",
            "41 0.9219\n",
            "42 0.9218\n",
            "43 0.9221\n",
            "44 0.9219\n",
            "45 0.922\n",
            "46 0.9219\n",
            "47 0.9219\n",
            "48 0.9221\n",
            "49 0.9221\n",
            "50 0.9219\n",
            "51 0.9219\n",
            "52 0.9221\n",
            "53 0.9223\n",
            "54 0.9223\n",
            "55 0.9224\n",
            "56 0.9224\n",
            "57 0.9224\n",
            "58 0.9226\n",
            "59 0.923\n",
            "60 0.9231\n",
            "61 0.9232\n",
            "62 0.9234\n",
            "63 0.9238\n",
            "64 0.9239\n",
            "65 0.9239\n",
            "66 0.9239\n",
            "67 0.9239\n",
            "68 0.9238\n",
            "69 0.924\n",
            "70 0.924\n",
            "71 0.9239\n",
            "72 0.9239\n",
            "73 0.9239\n",
            "74 0.9238\n",
            "75 0.9237\n",
            "76 0.9237\n",
            "77 0.9236\n",
            "78 0.9235\n",
            "79 0.9234\n",
            "80 0.9234\n",
            "81 0.9234\n",
            "82 0.9238\n",
            "83 0.9237\n",
            "84 0.9237\n",
            "85 0.9238\n",
            "86 0.9237\n",
            "87 0.9237\n",
            "88 0.9239\n",
            "89 0.9238\n",
            "90 0.9238\n",
            "91 0.9238\n",
            "92 0.9238\n",
            "93 0.9237\n",
            "94 0.9237\n",
            "95 0.9236\n",
            "96 0.9237\n",
            "97 0.9237\n",
            "98 0.9238\n",
            "99 0.9238\n",
            "100 0.9239\n",
            "101 0.9239\n",
            "102 0.9239\n",
            "103 0.9239\n",
            "104 0.9242\n",
            "105 0.9243\n",
            "106 0.9242\n",
            "107 0.9242\n",
            "108 0.9243\n",
            "109 0.9243\n",
            "110 0.9244\n",
            "111 0.9243\n",
            "112 0.9244\n",
            "113 0.9244\n",
            "114 0.9244\n",
            "115 0.9244\n",
            "116 0.9245\n",
            "117 0.9244\n",
            "118 0.9244\n",
            "119 0.9244\n",
            "120 0.9244\n",
            "121 0.9241\n",
            "122 0.9241\n",
            "123 0.9242\n",
            "124 0.9242\n",
            "125 0.9242\n",
            "126 0.9242\n",
            "127 0.924\n",
            "128 0.9239\n",
            "129 0.9239\n",
            "130 0.9238\n",
            "131 0.9238\n",
            "132 0.9238\n",
            "133 0.9239\n",
            "134 0.9239\n",
            "135 0.924\n",
            "136 0.9239\n",
            "137 0.9239\n",
            "138 0.9238\n",
            "139 0.9238\n",
            "140 0.9238\n",
            "141 0.9239\n",
            "142 0.9239\n",
            "143 0.9239\n",
            "144 0.924\n",
            "145 0.9239\n",
            "146 0.924\n",
            "147 0.9241\n",
            "148 0.9241\n",
            "149 0.9241\n",
            "150 0.924\n",
            "151 0.924\n",
            "152 0.9241\n",
            "153 0.9242\n",
            "154 0.9241\n",
            "155 0.9241\n",
            "156 0.9241\n",
            "157 0.9242\n",
            "158 0.9242\n",
            "159 0.9242\n",
            "160 0.9242\n",
            "161 0.9242\n",
            "162 0.9242\n",
            "163 0.9243\n",
            "164 0.9242\n",
            "165 0.9241\n",
            "166 0.9241\n",
            "167 0.9241\n",
            "168 0.9241\n",
            "169 0.9241\n",
            "170 0.9241\n",
            "171 0.924\n",
            "172 0.924\n",
            "173 0.924\n",
            "174 0.924\n",
            "175 0.924\n",
            "176 0.924\n",
            "177 0.9242\n",
            "178 0.9242\n",
            "179 0.9241\n",
            "180 0.9241\n",
            "181 0.9241\n",
            "182 0.9241\n",
            "183 0.9241\n",
            "184 0.924\n",
            "185 0.924\n",
            "186 0.924\n",
            "187 0.9241\n",
            "188 0.924\n",
            "189 0.9241\n",
            "190 0.9241\n",
            "191 0.924\n",
            "192 0.924\n",
            "193 0.9241\n",
            "194 0.9241\n",
            "195 0.9241\n",
            "196 0.9241\n",
            "197 0.9241\n",
            "198 0.9241\n",
            "199 0.9241\n",
            "200 0.9241\n",
            "201 0.9241\n",
            "202 0.9241\n",
            "203 0.9241\n",
            "204 0.9241\n",
            "205 0.9241\n",
            "206 0.9241\n",
            "207 0.9241\n",
            "208 0.9241\n",
            "209 0.9241\n",
            "210 0.9241\n",
            "211 0.9242\n",
            "212 0.9242\n",
            "213 0.9242\n",
            "214 0.9242\n",
            "215 0.9244\n",
            "216 0.9243\n",
            "217 0.9243\n",
            "218 0.9244\n",
            "219 0.9244\n",
            "220 0.9244\n",
            "221 0.9245\n",
            "222 0.9244\n",
            "223 0.9243\n",
            "224 0.9244\n",
            "225 0.9243\n",
            "226 0.9242\n",
            "227 0.9242\n",
            "228 0.9242\n",
            "229 0.9241\n",
            "230 0.9242\n",
            "231 0.9243\n",
            "232 0.9242\n",
            "233 0.9242\n",
            "234 0.9243\n",
            "235 0.9243\n",
            "236 0.9243\n",
            "237 0.9243\n",
            "238 0.9243\n",
            "239 0.9243\n",
            "240 0.9243\n",
            "241 0.9243\n",
            "242 0.9245\n",
            "243 0.9245\n",
            "244 0.9244\n",
            "245 0.9244\n",
            "246 0.9244\n",
            "247 0.9244\n",
            "248 0.9244\n",
            "249 0.9244\n",
            "250 0.9244\n",
            "251 0.9244\n",
            "252 0.9244\n",
            "253 0.9243\n",
            "254 0.9243\n",
            "255 0.9243\n",
            "256 0.9243\n",
            "257 0.9243\n",
            "258 0.9243\n",
            "259 0.9242\n",
            "260 0.9242\n",
            "261 0.9242\n",
            "262 0.9241\n",
            "263 0.9241\n",
            "264 0.9241\n",
            "265 0.9241\n",
            "266 0.9241\n",
            "267 0.9242\n",
            "268 0.9243\n",
            "269 0.9243\n",
            "270 0.9243\n",
            "271 0.9243\n",
            "272 0.9243\n",
            "273 0.9243\n",
            "274 0.9243\n",
            "275 0.9243\n",
            "276 0.9245\n",
            "277 0.9245\n",
            "278 0.9244\n",
            "279 0.9244\n",
            "280 0.9244\n",
            "281 0.9244\n",
            "282 0.9244\n",
            "283 0.9244\n",
            "284 0.9244\n",
            "285 0.9244\n",
            "286 0.9244\n",
            "287 0.9244\n",
            "288 0.9245\n",
            "289 0.9245\n",
            "290 0.9245\n",
            "291 0.9245\n",
            "292 0.9245\n",
            "293 0.9245\n",
            "294 0.9245\n",
            "295 0.9245\n",
            "296 0.9245\n",
            "297 0.9245\n",
            "298 0.9244\n",
            "299 0.9245\n",
            "300 0.9245\n",
            "301 0.9245\n",
            "302 0.9245\n",
            "303 0.9244\n",
            "304 0.9244\n",
            "305 0.9244\n",
            "306 0.9244\n",
            "307 0.9244\n",
            "308 0.9245\n",
            "309 0.9245\n",
            "310 0.9245\n",
            "311 0.9246\n",
            "312 0.9246\n",
            "313 0.9246\n",
            "314 0.9246\n",
            "315 0.9247\n",
            "316 0.9247\n",
            "317 0.9247\n",
            "318 0.9247\n",
            "319 0.9247\n",
            "320 0.9247\n",
            "321 0.9247\n",
            "322 0.9247\n",
            "323 0.9248\n",
            "324 0.9249\n",
            "325 0.925\n",
            "326 0.925\n",
            "327 0.925\n",
            "328 0.9251\n",
            "329 0.9251\n",
            "330 0.925\n",
            "331 0.925\n",
            "332 0.925\n",
            "333 0.925\n",
            "334 0.9251\n",
            "335 0.9251\n",
            "336 0.9251\n",
            "337 0.9251\n",
            "338 0.9251\n",
            "339 0.9251\n",
            "340 0.9251\n",
            "341 0.925\n",
            "342 0.925\n",
            "343 0.925\n",
            "344 0.925\n",
            "345 0.925\n",
            "346 0.9249\n",
            "347 0.9249\n",
            "348 0.9249\n",
            "349 0.9249\n",
            "350 0.9249\n",
            "351 0.9249\n",
            "352 0.9249\n",
            "353 0.9249\n",
            "354 0.925\n",
            "355 0.925\n",
            "356 0.9251\n",
            "357 0.9252\n",
            "358 0.9251\n",
            "359 0.9251\n",
            "360 0.925\n",
            "361 0.925\n",
            "362 0.9251\n",
            "363 0.9251\n",
            "364 0.925\n",
            "365 0.925\n",
            "366 0.9248\n",
            "367 0.9248\n",
            "368 0.9248\n",
            "369 0.9247\n",
            "370 0.9248\n",
            "371 0.925\n",
            "372 0.925\n",
            "373 0.925\n",
            "374 0.925\n",
            "375 0.925\n",
            "376 0.9251\n",
            "377 0.9251\n",
            "378 0.9251\n",
            "379 0.9251\n",
            "380 0.9251\n",
            "381 0.9251\n",
            "382 0.9251\n",
            "383 0.9252\n",
            "384 0.9252\n",
            "385 0.9251\n",
            "386 0.9251\n",
            "387 0.9251\n",
            "388 0.9251\n",
            "389 0.9251\n",
            "390 0.9251\n",
            "391 0.9251\n",
            "392 0.9251\n",
            "393 0.9251\n",
            "394 0.9251\n",
            "395 0.9251\n",
            "396 0.9251\n",
            "397 0.9251\n",
            "398 0.9252\n",
            "399 0.9252\n",
            "400 0.9252\n",
            "401 0.9252\n",
            "402 0.9252\n",
            "403 0.9251\n",
            "404 0.9251\n",
            "405 0.9251\n",
            "406 0.9251\n",
            "407 0.9252\n",
            "408 0.9252\n",
            "409 0.9252\n",
            "410 0.9252\n",
            "411 0.9252\n",
            "412 0.9253\n",
            "413 0.9253\n",
            "414 0.9253\n",
            "415 0.9253\n",
            "416 0.9253\n",
            "417 0.9253\n",
            "418 0.9253\n",
            "419 0.9253\n",
            "420 0.9253\n",
            "421 0.9253\n",
            "422 0.9253\n",
            "423 0.9252\n",
            "424 0.9252\n",
            "425 0.9252\n",
            "426 0.9252\n",
            "427 0.9252\n",
            "428 0.9252\n",
            "429 0.9252\n",
            "430 0.9252\n",
            "431 0.9252\n",
            "432 0.9252\n",
            "433 0.9252\n",
            "434 0.9252\n",
            "435 0.9251\n",
            "436 0.9251\n",
            "437 0.9251\n",
            "438 0.9251\n",
            "439 0.9251\n",
            "440 0.9252\n",
            "441 0.9252\n",
            "442 0.9252\n",
            "443 0.9252\n",
            "444 0.9253\n",
            "445 0.9253\n",
            "446 0.9253\n",
            "447 0.9253\n",
            "448 0.9253\n",
            "449 0.9252\n",
            "450 0.9252\n",
            "451 0.9252\n",
            "452 0.9252\n",
            "453 0.9252\n",
            "454 0.9253\n",
            "455 0.9253\n",
            "456 0.9253\n",
            "457 0.9252\n",
            "458 0.9252\n",
            "459 0.9252\n",
            "460 0.9252\n",
            "461 0.9252\n",
            "462 0.9252\n",
            "463 0.9252\n",
            "464 0.9252\n",
            "465 0.9252\n",
            "466 0.9252\n",
            "467 0.9252\n",
            "468 0.925\n",
            "469 0.925\n",
            "470 0.925\n",
            "471 0.925\n",
            "472 0.9249\n",
            "473 0.9249\n",
            "474 0.9249\n",
            "475 0.9249\n",
            "476 0.9248\n",
            "477 0.9248\n",
            "478 0.9247\n",
            "479 0.9247\n",
            "480 0.9248\n",
            "481 0.9248\n",
            "482 0.9248\n",
            "483 0.9248\n",
            "484 0.9248\n",
            "485 0.9248\n",
            "486 0.9248\n",
            "487 0.9248\n",
            "488 0.9248\n",
            "489 0.9248\n",
            "490 0.9248\n",
            "491 0.9248\n",
            "492 0.9247\n",
            "493 0.9247\n",
            "494 0.9247\n",
            "495 0.9247\n",
            "496 0.9247\n",
            "497 0.9247\n",
            "498 0.9247\n",
            "499 0.9246\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsxAFmx8Pjk-",
        "colab_type": "code",
        "outputId": "1a026419-0fbd-4e5d-9499-0e7cf89f4936",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1973
        }
      },
      "source": [
        "# SVM & RandomForest\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "#from sklearn.datasets import fetch_mldata\n",
        "X_train_MNIST, y_train_MNIST = Mnist_TrainingData, Mnist_TrainingTarget\n",
        "\n",
        "X_test_MNIST, y_test_MNIST = Mnist_TestingData, Mnist_TestingTarget\n",
        "X_test_USPS, y_test_USPS = USPS_TestingData, USPS_TargetData\n",
        "\n",
        "# SVM\n",
        "classifier1 = SVC(kernel='poly', C=2, gamma =0.01);#polynomial, increase C (tradeoff), remove gamma\n",
        "classifier1.fit(X_train_MNIST, y_train_MNIST)\n",
        "predicted_SVM_USPS = classifier1.predict(X_test_USPS)\n",
        "predicted_SVM_MNIST = classifier1.predict(X_test_MNIST)\n",
        "target_names = ['class 0', 'class 1', 'class 2','class 3', 'class 4', 'class 5','class 6', 'class 7', 'class 8','class 9']\n",
        "# get the accuracy\n",
        "print(\"SVM:\")\n",
        "print(\"Accuracy USPS\")\n",
        "print(classification_report(y_test_USPS, predicted_SVM_USPS, target_names=target_names))\n",
        "print(confusion_matrix(y_test_USPS, predicted_SVM_USPS))\n",
        "#print accuracy_score(y_test_USPS, predicted_SVM_USPS)\n",
        "print(\"Accuracy MNIST\")\n",
        "print(classification_report(y_test_MNIST, predicted_SVM_MNIST, target_names=target_names))\n",
        "print(confusion_matrix(y_test_MNIST, predicted_SVM_MNIST))\n",
        "#print accuracy_score(y_test_MNIST, predicted_SVM_MNIST)\n",
        "\n",
        "\n",
        "#RandomForestClassifier\n",
        "classifier2 = RandomForestClassifier(n_estimators=5);#less number of estimator, inlcude more parameters\n",
        "classifier2.fit(X_train_MNIST, y_train_MNIST)\n",
        "predicted_USPS = classifier2.predict(X_test_USPS)\n",
        "predicted_MNIST = classifier2.predict(X_test_MNIST)\n",
        "\n",
        "# get the accuracy\n",
        "print(\"Random Forest:\")\n",
        "print(\"Accuracy USPS\")\n",
        "print(classification_report(y_test_USPS, predicted_USPS, target_names=target_names))\n",
        "print(confusion_matrix(y_test_USPS, predicted_USPS))\n",
        "#print accuracy_score(y_test_USPS, predicted_SVM_USPS)\n",
        "print(\"Accuracy MNIST\")\n",
        "print(classification_report(y_test_MNIST, predicted_MNIST, target_names=target_names))\n",
        "print(confusion_matrix(y_test_MNIST, predicted_MNIST))\n",
        "#############################"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM:\n",
            "Accuracy USPS\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "    class 0       0.00      0.00      0.00      2000\n",
            "    class 1       0.10      1.00      0.18      2000\n",
            "    class 2       0.00      0.00      0.00      1999\n",
            "    class 3       0.00      0.00      0.00      2000\n",
            "    class 4       0.00      0.00      0.00      2000\n",
            "    class 5       0.00      0.00      0.00      2000\n",
            "    class 6       0.00      0.00      0.00      2000\n",
            "    class 7       0.00      0.00      0.00      2000\n",
            "    class 8       0.00      0.00      0.00      2000\n",
            "    class 9       0.00      0.00      0.00      2000\n",
            "\n",
            "avg / total       0.01      0.10      0.02     19999\n",
            "\n",
            "[[   0 2000    0    0    0    0    0    0    0    0]\n",
            " [   0 2000    0    0    0    0    0    0    0    0]\n",
            " [   0 1999    0    0    0    0    0    0    0    0]\n",
            " [   0 2000    0    0    0    0    0    0    0    0]\n",
            " [   0 2000    0    0    0    0    0    0    0    0]\n",
            " [   0 2000    0    0    0    0    0    0    0    0]\n",
            " [   0 2000    0    0    0    0    0    0    0    0]\n",
            " [   0 2000    0    0    0    0    0    0    0    0]\n",
            " [   0 2000    0    0    0    0    0    0    0    0]\n",
            " [   0 2000    0    0    0    0    0    0    0    0]]\n",
            "Accuracy MNIST\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "    class 0       0.98      0.99      0.99       991\n",
            "    class 1       0.98      0.99      0.99      1064\n",
            "    class 2       0.98      0.97      0.98       990\n",
            "    class 3       0.97      0.98      0.97      1030\n",
            "    class 4       0.99      0.98      0.98       983\n",
            "    class 5       0.98      0.96      0.97       915\n",
            "    class 6       0.98      0.99      0.99       967\n",
            "    class 7       0.98      0.98      0.98      1090\n",
            "    class 8       0.97      0.97      0.97      1009\n",
            "    class 9       0.97      0.96      0.97       961\n",
            "\n",
            "avg / total       0.98      0.98      0.98     10000\n",
            "\n",
            "[[ 984    0    3    0    0    0    1    1    1    1]\n",
            " [   0 1057    0    1    0    0    3    1    2    0]\n",
            " [   5    3  965    1    1    1    3    6    5    0]\n",
            " [   1    1    2 1005    1    5    0    3    9    3]\n",
            " [   0    5    2    0  960    0    0    2    1   13]\n",
            " [   1    0    5    8    1  880   11    0    4    5]\n",
            " [   1    1    0    0    1    2  961    0    1    0]\n",
            " [   3    6    1    2    2    1    0 1073    0    2]\n",
            " [   2    3    4   11    0    6    4    2  975    2]\n",
            " [   4    4    1    9    8    2    0    7    2  924]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Random Forest:\n",
            "Accuracy USPS\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "    class 0       0.14      0.07      0.09      2000\n",
            "    class 1       0.09      0.34      0.14      2000\n",
            "    class 2       0.18      0.21      0.19      1999\n",
            "    class 3       0.27      0.11      0.15      2000\n",
            "    class 4       0.38      0.06      0.11      2000\n",
            "    class 5       0.21      0.12      0.15      2000\n",
            "    class 6       0.00      0.00      0.00      2000\n",
            "    class 7       0.09      0.33      0.15      2000\n",
            "    class 8       0.00      0.00      0.00      2000\n",
            "    class 9       0.00      0.00      0.00      2000\n",
            "\n",
            "avg / total       0.14      0.12      0.10     19999\n",
            "\n",
            "[[ 134  852  152   12   30  135    0  685    0    0]\n",
            " [  66  690  213   63   12   14    0  942    0    0]\n",
            " [  83  663  421  102   21  128    0  581    0    0]\n",
            " [  92  739  250  215   19  119    0  565    0    1]\n",
            " [  74  659  102  132  124   25    0  883    0    1]\n",
            " [ 131  675  216   52    7  232    0  687    0    0]\n",
            " [  81  531  393   86   17  165    0  727    0    0]\n",
            " [  85 1050  163   14    7   27    0  654    0    0]\n",
            " [ 110  793  367   41   39  179    0  470    0    1]\n",
            " [  91  875   79   71   47   90    0  747    0    0]]\n",
            "Accuracy MNIST\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "    class 0       0.94      0.98      0.96       991\n",
            "    class 1       0.95      0.99      0.97      1064\n",
            "    class 2       0.90      0.93      0.91       990\n",
            "    class 3       0.87      0.93      0.90      1030\n",
            "    class 4       0.93      0.92      0.93       983\n",
            "    class 5       0.90      0.87      0.88       915\n",
            "    class 6       0.96      0.96      0.96       967\n",
            "    class 7       0.97      0.94      0.95      1090\n",
            "    class 8       0.93      0.86      0.89      1009\n",
            "    class 9       0.92      0.88      0.90       961\n",
            "\n",
            "avg / total       0.93      0.93      0.93     10000\n",
            "\n",
            "[[ 968    0    4    3    0    5    4    0    5    2]\n",
            " [   0 1050    3    1    3    3    0    1    3    0]\n",
            " [  14   15  917   14    7    0    4    7    7    5]\n",
            " [   6    2   23  955    0   19    1    4   15    5]\n",
            " [  10    6    4    4  907    4    7    3    2   36]\n",
            " [  10    7   15   55    6  793   12    2   12    3]\n",
            " [   8    0   10    3    5   11  928    0    1    1]\n",
            " [   3    7   11   13   11    6    0 1023    6   10]\n",
            " [   8   16   23   34    5   28    9    6  865   15]\n",
            " [   8    5    8   19   31   14    1   14   14  847]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjesAe7jPqQu",
        "colab_type": "code",
        "outputId": "16dc3f43-22b4-4086-c0d7-80a3f1ffee74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "# Neural Network\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Activation\n",
        "from keras.models import Sequential\n",
        "\n",
        "x_train_Mnist = Mnist_TrainingData\n",
        "y_train_Mnist = Mnist_TrainingTarget\n",
        "x_test_Mnist = Mnist_TestingData\n",
        "y_test_Mnist = Mnist_TestingTarget\n",
        "x_test_USPS = USPS_TestingData\n",
        "y_test_USPS = USPS_TargetData\n",
        "num_classes=20\n",
        "\n",
        "y_train_Mnist = keras.utils.to_categorical(y_train_Mnist, num_classes)\n",
        "y_test_Mnist = keras.utils.to_categorical(y_test_Mnist, num_classes)\n",
        "y_test_USPS = keras.utils.to_categorical(y_test_USPS, num_classes)\n",
        "image_size = 784\n",
        "model = Sequential()\n",
        "model.add(Dense(units=250, input_shape=(image_size,)))\n",
        "model.add(Activation(tf.nn.softmax))\n",
        "model.add(Dense(units=num_classes))\n",
        "model.add(Activation(tf.nn.softmax))\n",
        "classifiermodel.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.fit(x_train_Mnist, y_train_Mnist, batch_size=250, epochs=2000, verbose=False,validation_split=.1)\n",
        "#MNIST Testing\n",
        "loss,accuracy = model.evaluate(x_test_Mnist, y_test_Mnist, verbose=False)\n",
        "z1=accuracy\n",
        "print(\"MNIST Dataset:\")\n",
        "print(\"accuracy:\")\n",
        "print(accuracy)\n",
        "print(\"loss:\")\n",
        "print(loss)\n",
        "#print(confusion_matrix(x_test_Mnist, y_test_Mnist))\n",
        "#USPS Testing\n",
        "loss,accuracy = model.evaluate(x_test_USPS, y_test_USPS, verbose=False)\n",
        "z2=accuracy\n",
        "print(\"USPS Dataset:\")\n",
        "print(\"accuracy:\")\n",
        "print(accuracy)\n",
        "print(\"loss:\")\n",
        "print(loss)\n",
        "#print(confusion_matrix(x_test_USPS, y_test_USPS))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MNIST Dataset:\n",
            "accuracy:\n",
            "0.5895\n",
            "loss:\n",
            "0.8705210044860839\n",
            "USPS Dataset:\n",
            "accuracy:\n",
            "0.2627631381494569\n",
            "loss:\n",
            "2.6679038808574567\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxWZvvqMPxxX",
        "colab_type": "code",
        "outputId": "b0269a34-a133-4457-8d48-b192b7828b60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#MAJORITY VOTING USING VOTING CLASSIFIER\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "X=X_teest_MNIST\n",
        "Y=y_test_MNIST\n",
        "X1=X_test_USPS\n",
        "Y1=y_test_USPS\n",
        "clf1=LogisticRegression(solver='lbfgs', multi_class='multinomial',random_state=1)\n",
        "eclf1=VotingClassifier(estimators=[('lr', clf1), ('svm', classifier1), ('rf', classifier2), ('nn', z1) ], voting=soft)   #For MNIST\n",
        "eclf1=eclf1.fit(X, Y)\n",
        "print(eclf1.predict(X))\n",
        "print(eclf1.score(X, Y, sample_weight=None))\n",
        "eclf2=VotingClassifier(estimators=[('lr', clf1), ('svm', classifier1), ('rf', classifier2), ('nn', z2) ], voting=soft)   #For USPS\n",
        "eclf2=eclf2.fit(X1, Y1)\n",
        "print(eclf2.predict(X1))\n",
        "print(eclf2.score(X1, Y1, sample_weight=None))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Model.evaluate of <keras.engine.sequential.Sequential object at 0x7f7fd42d14e0>>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    }
  ]
}